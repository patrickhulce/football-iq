{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa10fcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df82d10d-56a5-456c-bb98-c9a8b572e0a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "image = read_image(\"../data/frames/pressbox-001.jpg\")\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df95281",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def quantize_image(image, n_colors):\n",
        "    channels, height, width,  = image.shape\n",
        "\n",
        "    # Reshape image to be a list of pixels\n",
        "    pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "    print(\"Pixels shape:\", pixels.shape) # [16384, 3]\n",
        "\n",
        "    unique_colors = torch.unique(pixels, dim=0) # Include all the unique colors\n",
        "    sampled_colors = shuffle(pixels, random_state=0, n_samples=100) # Sample 100 colors for frequency.\n",
        "    combined_colors = torch.vstack([unique_colors, sampled_colors]) # Combine the two tensors.\n",
        "\n",
        "    # Apply KMeans\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(combined_colors)\n",
        "    labels = kmeans.predict(pixels)\n",
        "    quantized = torch.from_numpy(kmeans.cluster_centers_[labels])\n",
        "    print(\"Quantized Shape:\", quantized.shape)\n",
        "    quantized = quantized.permute(1, 0).reshape(channels, height, width)\n",
        "    return quantized\n",
        "\n",
        "def blur_image_manual(image: torch.Tensor, kernel_size=3):\n",
        "    # Define the blur kernel\n",
        "    blur_kernel = torch.ones(1, 1, kernel_size, kernel_size) / (kernel_size * kernel_size)\n",
        "    blur_kernel = blur_kernel.repeat(image.shape[0], 1, 1, 1)  # Repeat for each input channel\n",
        "\n",
        "    # Add an extra dimension to the image tensor and apply blur\n",
        "    image = image.unsqueeze(0)  # Add extra dimension for batch size\n",
        "    blurred_image = F.conv2d(image, blur_kernel, padding=1, groups=3)\n",
        "\n",
        "    # Remove the extra dimension\n",
        "    return blurred_image.squeeze(0)\n",
        "\n",
        "def blur_image(image: torch.Tensor, kernel_size=3):\n",
        "    return transforms.GaussianBlur(kernel_size=kernel_size)(image)\n",
        "\n",
        "def morphological_closing(image: torch.Tensor, kernel_size=2):\n",
        "    # Convert the tensor to a NumPy array\n",
        "    image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
        "    img = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Define a kernel for the morphological operation\n",
        "    kernel = np.ones((kernel_size,kernel_size), np.uint8)  # you may need to adjust the size\n",
        "\n",
        "    # Get all unique colors in the image\n",
        "    unique_colors = np.unique(img.reshape(-1, img.shape[2]), axis=0)\n",
        "\n",
        "    # Perform morphological closing for each unique color\n",
        "    cleaned_img = np.zeros_like(img)\n",
        "    for color in unique_colors:\n",
        "        # Create a binary mask for the current color\n",
        "        print(\"Processing color:\", color)\n",
        "        mask = (img == color).all(axis=2).astype(np.uint8)\n",
        "\n",
        "        # Perform morphological closing on the mask\n",
        "        closed_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "        # closed_mask = cv2.morphologyEx(closed_mask, cv2.MORPH_OPEN, kernel)\n",
        "        # closed_mask = cv2.dilate(closed_mask, kernel, iterations=1)\n",
        "\n",
        "        # Only keep the largest connected component by pixel count.\n",
        "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(closed_mask, connectivity=8)\n",
        "        # Order the labels by number of active pixels.\n",
        "        sorted_labels = np.argsort(stats[:, cv2.CC_STAT_AREA])\n",
        "        max_label = sorted_labels[-2]\n",
        "\n",
        "        for label in range(1, num_labels):\n",
        "            label_area = stats[label, cv2.CC_STAT_AREA]\n",
        "            if label_area < 50:\n",
        "                continue\n",
        "            # If the component isn't large enough, remove it from the cleaned image\n",
        "            print(\"Component area of {} comapred to max area of {}\".format(stats[label, cv2.CC_STAT_AREA], stats[max_label, cv2.CC_STAT_AREA]))\n",
        "            if label_area < stats[max_label, cv2.CC_STAT_AREA]:\n",
        "                closed_mask[labels == label] = 0\n",
        "\n",
        "        # Add the closed mask to the cleaned image\n",
        "        cleaned_img[closed_mask == 1] = color\n",
        "\n",
        "    # Convert the cleaned NumPy array back to a tensor\n",
        "    cleaned_tensor = torch.from_numpy(cleaned_img).permute(2, 0, 1)\n",
        "\n",
        "    return cleaned_tensor / 255.0\n",
        "\n",
        "def quantize_image_by_popularity(image, min_popularity: int = 150, use_blur: bool = False, use_morphology: bool = True):\n",
        "    channels, height, width = image.shape\n",
        "\n",
        "    # Reshape image to be a list of pixels\n",
        "    # image = (image * 255).clamp(0, 255).to(torch.int32)\n",
        "    # image = blur_image(image, kernel_size=3) if use_blur else image\n",
        "    # pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "    # print(\"Pixels shape:\", pixels.shape) # [16384, 3]\n",
        "\n",
        "    # # Count the number of times each color appears in the image.\n",
        "    # pixel_ints = pixels\n",
        "\n",
        "    image = blur_image(image, kernel_size=3) if use_blur else image\n",
        "    pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "\n",
        "    # Count the number of times each color appears in the image.\n",
        "    pixel_ints = (pixels * 255).to(torch.int32)\n",
        "    # Treat any pixel with R, G, and B combined <100 as black.\n",
        "    pixel_ints[pixel_ints.sum(dim=1) < 100] = 0\n",
        "    # Treat any pixel with R, G, and B combined >710 as white.\n",
        "    pixel_ints[pixel_ints.sum(dim=1) > 710] = 255\n",
        "\n",
        "    pixel_merged = pixel_ints[:, 0] * 256 * 256 + pixel_ints[:, 1] * 256 + pixel_ints[:, 2]\n",
        "\n",
        "\n",
        "    print(\"Pixels merged shape:\", pixel_merged.shape)\n",
        "    print(\"Pixels merged:\", pixel_merged[:10])\n",
        "    unique_color_merged_counts = torch.bincount(pixel_merged)\n",
        "    print(\"Unique color merged counts shape:\", unique_color_merged_counts.shape)\n",
        "    print(\"Unique color merged counts:\", unique_color_merged_counts[:10])\n",
        "    sorted_indices = unique_color_merged_counts.argsort(descending=True)\n",
        "    top_colors_merged = sorted_indices[:30]\n",
        "    # Restrict top colors to those that appear at least min_popularity times.\n",
        "    top_color_counts = unique_color_merged_counts[top_colors_merged]\n",
        "    top_colors_merged = top_colors_merged[top_color_counts >= min_popularity]\n",
        "    top_colors = [(color.item() // (256 * 256), (color.item() // 256) % 256, color.item() % 256) for color in top_colors_merged] + [(255, 255, 255)]\n",
        "    top_colors_tensor = torch.tensor(top_colors, dtype=torch.float32) / 255  # Convert to tensor and normalize to [0, 1]\n",
        "    print(\"Top colors:\", top_colors)\n",
        "    print(\"Top color counts:\", top_color_counts)\n",
        "\n",
        "    # Remap each pixel in the image to the closest of the top colors.\n",
        "    distances = torch.norm(pixels.unsqueeze(1) - top_colors_tensor, dim=2)  # Calculate distances to top colors\n",
        "    closest = distances.argmin(dim=1)  # Find the index of the smallest distance\n",
        "    remapped_pixels = top_colors_tensor[closest]  # Use this index to get the corresponding top color\n",
        "\n",
        "    # Reshape the remapped pixels to the original image shape\n",
        "    remapped_image = remapped_pixels.permute(1, 0).reshape(channels, height, width)\n",
        "\n",
        "    return remapped_image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mask = read_image(\"../data/frames/madden-001-mask.jpg\")\n",
        "mask = mask / 255.0\n",
        "print(\"Mask shape:\", mask.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.subplot(221)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))\n",
        "plt.subplot(222)\n",
        "plt.title(\"Blurred\")\n",
        "plt.imshow(blur_image(mask, kernel_size=3).permute(1, 2, 0))\n",
        "plt.subplot(223)\n",
        "plt.title(\"Quantized by popularity\")\n",
        "plt.imshow(quantize_image_by_popularity(mask, 500).permute(1, 2, 0))\n",
        "plt.subplot(224)\n",
        "plt.title(\"Quantized by k-means\")\n",
        "plt.imshow(quantize_image(mask, 24).permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e26028",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Define the size of the image\n",
        "height = 64\n",
        "width = 128\n",
        "\n",
        "# Create a tensor with values increasing linearly between 0 and 1\n",
        "image_hsv = torch.zeros((3, height, width))\n",
        "image_hsv[0] = torch.linspace(0, 1, height * width).view(height, width)\n",
        "\n",
        "# Set saturation and value to 1\n",
        "image_hsv[1] = 1\n",
        "image_hsv[2] = 1\n",
        "\n",
        "# Convert the image to RGB\n",
        "image_rgb = mcolors.hsv_to_rgb(image_hsv.permute(1, 2, 0).numpy())\n",
        "\n",
        "# Convert back to tensor\n",
        "image_rgb_tensor = torch.from_numpy(image_rgb)\n",
        "print(\"RGB Image Shape:\", image_rgb_tensor.shape)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image_rgb_tensor)\n",
        "plt.subplot(122)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(quantize_image(image_rgb_tensor.permute(2, 0, 1), 8).permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f308c37d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Shape:\", image_rgb_tensor.shape)\n",
        "pixels = image_rgb_tensor.reshape(-1, 3)\n",
        "print(\"Shape:\", pixels.shape)\n",
        "print(\"First 10 pixels:\\n\", pixels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_dir = \"../data/frames/\"\n",
        "\n",
        "import os\n",
        "from torchvision.io import read_image, write_png\n",
        "from PIL import Image\n",
        "\n",
        "for filename in os.listdir(target_dir):\n",
        "    if filename.endswith('-mask.jpg'):\n",
        "        print(\"Processing\", filename, \"...\")\n",
        "        # Open image and quantize it\n",
        "        image = read_image(os.path.join(target_dir, filename))\n",
        "        image = quantize_image_by_popularity(image / 255.0, 500)\n",
        "\n",
        "        # Print the first 10 pixels that aren't black\n",
        "        indices = (image.view(3, -1).sum(0) != 0).nonzero(as_tuple=True)[0]\n",
        "\n",
        "        # Transform flat indices back to 2D coordinates\n",
        "        y = torch.div(indices[:10], image.shape[2]).to(torch.int32)\n",
        "        x = torch.fmod(indices[:10], image.shape[2]).to(torch.int32)\n",
        "\n",
        "        # Print the first 10 non-black pixels and their coordinates\n",
        "        for idx in range(10):\n",
        "            print(f\"Pixel: {image[:, y[idx], x[idx]]}, Coordinates: ({y[idx].item()}, {x[idx].item()})\")\n",
        "\n",
        "        # Save quantized image as PNG\n",
        "        int_image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
        "\n",
        "        new_filename = os.path.splitext(filename)[0] + '.png'\n",
        "        write_png(int_image, os.path.join(target_dir, new_filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9bd545f-9264-4f98-bdca-36889145685e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "\n",
        "class FootballDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, frame_directory, transforms):\n",
        "        self.frame_directory = frame_directory\n",
        "        self.transforms = transforms\n",
        "        images = list(sorted(os.listdir(frame_directory)))\n",
        "        self.imgs = [img for img in images if not img.endswith(\"-mask.jpg\")]\n",
        "        self.masks = [img for img in images if img.endswith(\"-mask.jpg\")]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.frame_directory, self.imgs[idx])\n",
        "        mask_path = os.path.join(self.frame_directory, self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
