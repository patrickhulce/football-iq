{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa10fcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df82d10d-56a5-456c-bb98-c9a8b572e0a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "image = read_image(\"../data/frames/pressbox-001.jpg\")\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df95281",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def quantize_image(image, n_colors):\n",
        "    channels, height, width,  = mask.shape\n",
        "\n",
        "    # Reshape image to be a list of pixels\n",
        "    pixels = mask.reshape(channels, -1).T\n",
        "    unique_colors = torch.unique(pixels, dim=1).T\n",
        "    sampled_colors = shuffle(pixels, random_state=0, n_samples=100)\n",
        "    combined_colors = torch.vstack([unique_colors, sampled_colors])\n",
        "\n",
        "    # Apply KMeans\n",
        "    kmeans = KMeans(n_clusters=n_colors)\n",
        "    labels = kmeans.fit_predict(combined_colors)\n",
        "    print(\"Labels:\", labels)\n",
        "    # image = kmeans.cluster_centers_[labels].reshape(-1, 3)\n",
        "\n",
        "    # # Convert back to PIL Image\n",
        "    # image = transforms.ToPILImage()(image.clamp(0, 1).view(-1, height, width))\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "mask = mask / 255.0\n",
        "print(\"Mask shape:\", mask.shape)\n",
        "quantized_mask = quantize_image(mask, 24)\n",
        "print(\"Quantized mask shape:\", quantized_mask.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Quantized\")\n",
        "plt.imshow(quantized_mask.permute(1, 2, 0))\n",
        "\n",
        "# for filename in os.listdir(path):\n",
        "#     if filename.endswith('.jpeg') or filename.endswith('.jpg'):\n",
        "#         # Open image and quantize it\n",
        "#         image = Image.open(os.path.join(path, filename))\n",
        "#         image = quantize_image(image, 24)\n",
        "        \n",
        "#         # Save quantized image as PNG\n",
        "#         new_filename = os.path.splitext(filename)[0] + '.png'\n",
        "#         image.save(os.path.join(path, new_filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9bd545f-9264-4f98-bdca-36889145685e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "\n",
        "class FootballDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, frame_directory, transforms):\n",
        "        self.frame_directory = frame_directory\n",
        "        self.transforms = transforms\n",
        "        images = list(sorted(os.listdir(frame_directory)))\n",
        "        self.imgs = [img for img in images if not img.endswith(\"-mask.jpg\")]\n",
        "        self.masks = [img for img in images if img.endswith(\"-mask.jpg\")]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.frame_directory, self.imgs[idx])\n",
        "        mask_path = os.path.join(self.frame_directory, self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        \n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
