{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa10fcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df82d10d-56a5-456c-bb98-c9a8b572e0a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "image = read_image(\"../data/frames/pressbox-001.jpg\")\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df95281",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def quantize_image(image, n_colors):\n",
        "    channels, height, width,  = image.shape\n",
        "\n",
        "    # Reshape image to be a list of pixels\n",
        "    pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "    print(\"Pixels shape:\", pixels.shape) # [16384, 3]\n",
        "\n",
        "    unique_colors = torch.unique(pixels, dim=0) # Include all the unique colors\n",
        "    sampled_colors = shuffle(pixels, random_state=0, n_samples=100) # Sample 100 colors for frequency.\n",
        "    combined_colors = torch.vstack([unique_colors, sampled_colors]) # Combine the two tensors.\n",
        "\n",
        "    # Apply KMeans\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(combined_colors)\n",
        "    labels = kmeans.predict(pixels)\n",
        "    quantized = torch.from_numpy(kmeans.cluster_centers_[labels])\n",
        "    print(\"Quantized Shape:\", quantized.shape)\n",
        "    quantized = quantized.permute(1, 0).reshape(channels, height, width)\n",
        "    return quantized\n",
        "\n",
        "def blur_image_manual(image: torch.Tensor, kernel_size=3):\n",
        "    # Define the blur kernel\n",
        "    blur_kernel = torch.ones(1, 1, kernel_size, kernel_size) / (kernel_size * kernel_size)\n",
        "    blur_kernel = blur_kernel.repeat(image.shape[0], 1, 1, 1)  # Repeat for each input channel\n",
        "\n",
        "    # Add an extra dimension to the image tensor and apply blur\n",
        "    image = image.unsqueeze(0)  # Add extra dimension for batch size\n",
        "    blurred_image = F.conv2d(image, blur_kernel, padding=1, groups=3)\n",
        "\n",
        "    # Remove the extra dimension\n",
        "    return blurred_image.squeeze(0)\n",
        "\n",
        "def blur_image(image: torch.Tensor, kernel_size=3):\n",
        "    return transforms.GaussianBlur(kernel_size=kernel_size)(image)\n",
        "\n",
        "def morphological_closing(image: torch.Tensor, kernel_size=2):\n",
        "    # Convert the tensor to a NumPy array\n",
        "    image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
        "    img = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Define a kernel for the morphological operation\n",
        "    kernel = np.ones((kernel_size,kernel_size), np.uint8)  # you may need to adjust the size\n",
        "\n",
        "    # Get all unique colors in the image\n",
        "    unique_colors = np.unique(img.reshape(-1, img.shape[2]), axis=0)\n",
        "\n",
        "    # Perform morphological closing for each unique color\n",
        "    cleaned_img = np.zeros_like(img)\n",
        "    for color in unique_colors:\n",
        "        # Create a binary mask for the current color\n",
        "        print(\"Processing color:\", color)\n",
        "        mask = (img == color).all(axis=2).astype(np.uint8)\n",
        "\n",
        "        # Perform morphological closing on the mask\n",
        "        closed_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "        # closed_mask = cv2.morphologyEx(closed_mask, cv2.MORPH_OPEN, kernel)\n",
        "        # closed_mask = cv2.dilate(closed_mask, kernel, iterations=1)\n",
        "\n",
        "        # Only keep the largest connected component by pixel count.\n",
        "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(closed_mask, connectivity=8)\n",
        "        # Order the labels by number of active pixels.\n",
        "        sorted_labels = np.argsort(stats[:, cv2.CC_STAT_AREA])\n",
        "        max_label = sorted_labels[-2]\n",
        "\n",
        "        for label in range(1, num_labels):\n",
        "            label_area = stats[label, cv2.CC_STAT_AREA]\n",
        "            if label_area < 50:\n",
        "                continue\n",
        "            # If the component isn't large enough, remove it from the cleaned image\n",
        "            print(\"Component area of {} comapred to max area of {}\".format(stats[label, cv2.CC_STAT_AREA], stats[max_label, cv2.CC_STAT_AREA]))\n",
        "            if label_area < stats[max_label, cv2.CC_STAT_AREA]:\n",
        "                closed_mask[labels == label] = 0\n",
        "\n",
        "        # Add the closed mask to the cleaned image\n",
        "        cleaned_img[closed_mask == 1] = color\n",
        "\n",
        "    # Convert the cleaned NumPy array back to a tensor\n",
        "    cleaned_tensor = torch.from_numpy(cleaned_img).permute(2, 0, 1)\n",
        "\n",
        "    return cleaned_tensor / 255.0\n",
        "\n",
        "def quantize_image_by_popularity(image, min_popularity: int = 150, use_blur: bool = False, use_morphology: bool = True):\n",
        "    channels, height, width = image.shape\n",
        "\n",
        "    # Reshape image to be a list of pixels\n",
        "    # image = (image * 255).clamp(0, 255).to(torch.int32)\n",
        "    # image = blur_image(image, kernel_size=3) if use_blur else image\n",
        "    # pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "    # print(\"Pixels shape:\", pixels.shape) # [16384, 3]\n",
        "\n",
        "    # # Count the number of times each color appears in the image.\n",
        "    # pixel_ints = pixels\n",
        "\n",
        "    image = blur_image(image, kernel_size=3) if use_blur else image\n",
        "    pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "\n",
        "    # Count the number of times each color appears in the image.\n",
        "    pixel_ints = (pixels * 255).to(torch.int32)\n",
        "    # Treat any pixel with R, G, and B combined <100 as black.\n",
        "    pixel_ints[pixel_ints.sum(dim=1) < 100] = 0\n",
        "    # Treat any pixel with R, G, and B combined >710 as white.\n",
        "    pixel_ints[pixel_ints.sum(dim=1) > 710] = 255\n",
        "\n",
        "    pixel_merged = pixel_ints[:, 0] * 256 * 256 + pixel_ints[:, 1] * 256 + pixel_ints[:, 2]\n",
        "\n",
        "\n",
        "    print(\"Pixels merged shape:\", pixel_merged.shape)\n",
        "    print(\"Pixels merged:\", pixel_merged[:10])\n",
        "    unique_color_merged_counts = torch.bincount(pixel_merged)\n",
        "    print(\"Unique color merged counts shape:\", unique_color_merged_counts.shape)\n",
        "    print(\"Unique color merged counts:\", unique_color_merged_counts[:10])\n",
        "    sorted_indices = unique_color_merged_counts.argsort(descending=True)\n",
        "    top_colors_merged = sorted_indices[:30]\n",
        "    # Restrict top colors to those that appear at least min_popularity times.\n",
        "    top_color_counts = unique_color_merged_counts[top_colors_merged]\n",
        "    top_colors_merged = top_colors_merged[top_color_counts >= min_popularity]\n",
        "    top_colors = [(color.item() // (256 * 256), (color.item() // 256) % 256, color.item() % 256) for color in top_colors_merged] + [(255, 255, 255)]\n",
        "    top_colors_tensor = torch.tensor(top_colors, dtype=torch.float32) / 255  # Convert to tensor and normalize to [0, 1]\n",
        "    print(\"Top colors:\", top_colors)\n",
        "    print(\"Top color counts:\", top_color_counts)\n",
        "\n",
        "    # Remap each pixel in the image to the closest of the top colors.\n",
        "    distances = torch.norm(pixels.unsqueeze(1) - top_colors_tensor, dim=2)  # Calculate distances to top colors\n",
        "    closest = distances.argmin(dim=1)  # Find the index of the smallest distance\n",
        "    remapped_pixels = top_colors_tensor[closest]  # Use this index to get the corresponding top color\n",
        "\n",
        "    # Reshape the remapped pixels to the original image shape\n",
        "    remapped_image = remapped_pixels.permute(1, 0).reshape(channels, height, width)\n",
        "    remapped_image = morphological_closing(remapped_image, kernel_size=5) if use_morphology else image\n",
        "    return remapped_image\n",
        "\n",
        "def quantized_by_edges(image):\n",
        "    # Convert the tensor to a NumPy array\n",
        "    image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
        "    img = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Split the image into R, G, B channels\n",
        "    r, g, b = cv2.split(img)\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Perform Canny edge detection on each channel\n",
        "    bottom_threshold = 25\n",
        "    top_threshold = 100\n",
        "    edges_k = cv2.Canny(img_gray, bottom_threshold, top_threshold)\n",
        "    edges_r = cv2.Canny(r, bottom_threshold, top_threshold)\n",
        "    edges_g = cv2.Canny(g, bottom_threshold, top_threshold)\n",
        "    edges_b = cv2.Canny(b, bottom_threshold, top_threshold)\n",
        "\n",
        "    # cv2.imwrite(\"edges_k.jpg\", edges_k)\n",
        "    # cv2.imwrite(\"edges_g.jpg\", edges_g)\n",
        "    # Combine the edges from all channels\n",
        "    _, edges = cv2.threshold(edges_k | edges_r | edges_g | edges_b, 25, 255, cv2.THRESH_BINARY)\n",
        "    edges_pre_dialation = edges.copy()\n",
        "    edges = cv2.dilate(edges, np.ones((3, 3), np.uint8), iterations=1)\n",
        "    edges = cv2.erode(edges, np.ones((3, 3), np.uint8), iterations=1)\n",
        "    edges_post_dialation = edges.copy()\n",
        "    # Save the dilated regions for later to bring back some of the image.\n",
        "    dilated_regions = (edges_post_dialation - edges_pre_dialation) > 10\n",
        "    # cv2.imwrite(\"edges.jpg\", edges)\n",
        "\n",
        "    # Calculate the intensity of the original image\n",
        "    intensities = np.mean(img, axis=2)\n",
        "    # Create a mask of dark colors in the original image\n",
        "    bg_mask = intensities < 30\n",
        "\n",
        "    # Invert the edges mask and reapply the background mask\n",
        "    edges = cv2.bitwise_not(edges)\n",
        "    edges[bg_mask == 1] = 0\n",
        "\n",
        "    # Find connected components from edges\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(edges, connectivity=4)\n",
        "\n",
        "    # Create an empty image to store the result\n",
        "    result = np.zeros_like(img)\n",
        "\n",
        "    # Compute a mask that is white where any bounding box of a connected component intersects with another component.\n",
        "    # This will be used to remove dilated regions that are too close to each other.\n",
        "    component_intersections_tmp = np.ones_like(edges, dtype=np.int32)\n",
        "    FIXED_OFFSET = 500\n",
        "    for label in range(1, num_labels):\n",
        "        x, y, w, h = stats[label, cv2.CC_STAT_LEFT], stats[label, cv2.CC_STAT_TOP], \\\n",
        "                     stats[label, cv2.CC_STAT_WIDTH], stats[label, cv2.CC_STAT_HEIGHT]\n",
        "        # x = max(0, x - 15)\n",
        "        # y = max(0, y - 15)\n",
        "        # h = min(component_intersections_tmp.shape[0] - y, h + 30)\n",
        "        # w = min(component_intersections_tmp.shape[1] - x, w + 30)\n",
        "        component_intersections_tmp[y:y+h, x:x+w] *= FIXED_OFFSET + label\n",
        "    component_intersections = component_intersections_tmp > (FIXED_OFFSET + num_labels)\n",
        "    component_indices = component_intersections_tmp.copy()\n",
        "    component_indices[component_intersections] = -1\n",
        "    component_indices -= FIXED_OFFSET\n",
        "    component_indices[component_indices < 0] = -1\n",
        "    component_indices[~dilated_regions] = -1\n",
        "\n",
        "\n",
        "    # Iterate over each label found in the image\n",
        "    for label in range(1, num_labels):\n",
        "        should_print = False\n",
        "        if stats[label, cv2.CC_STAT_LEFT] > 400 and stats[label, cv2.CC_STAT_LEFT] < 800 and stats[label, cv2.CC_STAT_TOP] > 400 and stats[label, cv2.CC_STAT_TOP] < 800:\n",
        "            should_print = False\n",
        "\n",
        "        log = lambda *args: print(*args) if should_print else None\n",
        "        if stats[label, cv2.CC_STAT_AREA] < 30:\n",
        "            continue\n",
        "        object_mask = (labels == label).astype(np.uint8)\n",
        "        aux_mask = (component_indices == label).astype(np.uint8)\n",
        "        # cv2.imwrite(f\"object_mask-{label}.jpg\", aux_mask * 255)\n",
        "        object_mask += aux_mask\n",
        "\n",
        "\n",
        "        # Get the bounding box of the current component\n",
        "        x, y, w, h = stats[label, cv2.CC_STAT_LEFT], stats[label, cv2.CC_STAT_TOP], \\\n",
        "                     stats[label, cv2.CC_STAT_WIDTH], stats[label, cv2.CC_STAT_HEIGHT]\n",
        "        log(\"Processing label {} with bounding box ({}, {}, {}, {})\".format(label, x, y, w, h))\n",
        "        mask = np.zeros_like(edges, dtype=np.uint8)\n",
        "        mask[y-1:y+h+1, x-1:x+w+1] = 1\n",
        "        log(\"Image shape:\", img.shape)\n",
        "        log(\"Mask shape:\", mask.shape)\n",
        "\n",
        "        # Stack the colors together along the last axis\n",
        "        colors = img[object_mask == 1]\n",
        "        log(\"Colors shape:\", colors.shape)\n",
        "\n",
        "        # Find unique colors and their counts\n",
        "        unique_colors, counts = np.unique(colors, return_counts=True, axis=0)\n",
        "        # Calculate the intensity of each unique color\n",
        "        intensities = np.mean(unique_colors, axis=1)\n",
        "        # Create a mask that includes only the colors with intensity above a certain threshold\n",
        "        colors_mask = intensities > 30\n",
        "        # Apply the mask to unique_colors and counts\n",
        "        unique_colors = unique_colors[colors_mask]\n",
        "        counts = counts[colors_mask]\n",
        "\n",
        "        # Get the color with the maximum count\n",
        "        most_common_color = unique_colors[np.argmax(counts)]\n",
        "\n",
        "        # Convert most_common_color to tuple for floodFill\n",
        "        color_tuple = (int(most_common_color[0]), int(most_common_color[1]), int(most_common_color[2]))\n",
        "\n",
        "        # Get the seed point\n",
        "        seed_point = (int(centroids[label][0]), int(centroids[label][1]))\n",
        "\n",
        "        # Create a mask for flood filling that is 2 pixels larger in each direction than mask.\n",
        "        flood_fill_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
        "        # now fill the same pixels as were in mask but shifted 1 pixel in each direction\n",
        "        flood_fill_mask[1:-1, 1:-1] = mask\n",
        "\n",
        "        log(f\"Seed point: {seed_point}, color: {color_tuple}\")\n",
        "        # Fill the component with the most common color in the result image\n",
        "        result[object_mask == 1] = most_common_color\n",
        "        # Perform flood fill with the most common color\n",
        "        # cv2.floodFill(result, flood_fill_mask, seedPoint=seed_point, newVal=color_tuple)\n",
        "\n",
        "    # Convert the result NumPy array back to a tensor\n",
        "    result_tensor = torch.from_numpy(result).permute(2, 0, 1) / 255.0\n",
        "\n",
        "    result_tensor = morphological_closing(result_tensor, kernel_size=5)\n",
        "\n",
        "    # Convert edges to an RGB image\n",
        "    edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # Convert the RGB numpy array to a PyTorch tensor\n",
        "    edges_tensor = torch.from_numpy(edges_rgb).permute(2, 0, 1) / 255.0\n",
        "\n",
        "    # return edges_tensor\n",
        "    return result_tensor\n",
        "\n",
        "def image_for_display(image, crop=False):\n",
        "    # Define the crop coordinates\n",
        "    start_row = 500\n",
        "    start_col = 500\n",
        "    height = 200\n",
        "    width = 400\n",
        "\n",
        "    # Crop the image\n",
        "    cropped_img = image[:, start_row:start_row+height, start_col:start_col+width]\n",
        "\n",
        "    image = cropped_img if crop else image\n",
        "    return image.permute(1, 2, 0)\n",
        "\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "mask = mask / 255.0\n",
        "print(\"Mask shape:\", mask.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.subplot(211)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(image_for_display(mask))\n",
        "# plt.subplot(222)\n",
        "# plt.title(\"Blurred\")\n",
        "# plt.imshow(image_for_display(blur_image(mask, kernel_size=3)))\n",
        "# plt.subplot(223)\n",
        "# plt.title(\"Quantized by popularity\")\n",
        "# plt.imshow(image_for_display(quantize_image_by_popularity(mask, min_popularity=300, use_morphology=True)))\n",
        "plt.subplot(212)\n",
        "plt.title(\"Quantized by edges\")\n",
        "plt.imshow(image_for_display(quantized_by_edges(mask)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e26028",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Define the size of the image\n",
        "height = 64\n",
        "width = 128\n",
        "\n",
        "# Create a tensor with values increasing linearly between 0 and 1\n",
        "image_hsv = torch.zeros((3, height, width))\n",
        "image_hsv[0] = torch.linspace(0, 1, height * width).view(height, width)\n",
        "\n",
        "# Set saturation and value to 1\n",
        "image_hsv[1] = 1\n",
        "image_hsv[2] = 1\n",
        "\n",
        "# Convert the image to RGB\n",
        "image_rgb = mcolors.hsv_to_rgb(image_hsv.permute(1, 2, 0).numpy())\n",
        "\n",
        "# Convert back to tensor\n",
        "image_rgb_tensor = torch.from_numpy(image_rgb)\n",
        "print(\"RGB Image Shape:\", image_rgb_tensor.shape)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image_rgb_tensor)\n",
        "plt.subplot(122)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(quantize_image(image_rgb_tensor.permute(2, 0, 1), 8).permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f308c37d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Shape:\", image_rgb_tensor.shape)\n",
        "pixels = image_rgb_tensor.reshape(-1, 3)\n",
        "print(\"Shape:\", pixels.shape)\n",
        "print(\"First 10 pixels:\\n\", pixels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_dir = \"../data/frames/\"\n",
        "force = True\n",
        "\n",
        "import os\n",
        "from torchvision.io import read_image, write_png\n",
        "from PIL import Image\n",
        "\n",
        "for filename in os.listdir(target_dir):\n",
        "    if filename.endswith('-mask.jpg'):\n",
        "        # Skip the file if the equivalent png file already exists\n",
        "        new_filename = os.path.splitext(filename)[0] + '.png'\n",
        "        if os.path.isfile(os.path.join(target_dir, new_filename)) and not force:\n",
        "            continue\n",
        "\n",
        "        print(\"Processing\", filename, \"...\")\n",
        "        # Open image and quantize it\n",
        "        image = read_image(os.path.join(target_dir, filename))\n",
        "        image = quantized_by_edges(image / 255.0)\n",
        "\n",
        "        # Save quantized image as PNG\n",
        "        int_image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
        "\n",
        "        write_png(int_image, os.path.join(target_dir, new_filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0242fb3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914afc80",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "import importlib\n",
        "import process_mask\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "importlib.reload(process_mask)\n",
        "\n",
        "mask = read_image(\"../data/frames/madden-001-mask.png\")\n",
        "target = process_mask.process_mask(mask, 1)\n",
        "\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(mask.permute(1, 2, 0))\n",
        "print(target['labels'])\n",
        "\n",
        "\n",
        "boxes = target[\"boxes\"].numpy()\n",
        "masks = target[\"masks\"].numpy()\n",
        "labels = target[\"labels\"].numpy()\n",
        "\n",
        "# Iterate over each object\n",
        "for i in range(len(labels)):\n",
        "    # Draw the bounding box\n",
        "    box = boxes[i]\n",
        "    rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    # Draw the mask\n",
        "    mask = masks[i]\n",
        "    mask_rgba = np.zeros((mask.shape[0], mask.shape[1], 4))  # Initialize with zeros\n",
        "    mask_rgba[masks[i] > 0, :3] = 1  # Set RGB channels to 1 where mask is not zero\n",
        "    mask_rgba[masks[i] > 0, 3] = 1  # Set Alpha channel to 1 where mask is not zero\n",
        "\n",
        "    size_of_mask = np.where(mask == True)[0].sum()\n",
        "\n",
        "    print(f\"Size of mask for label {labels[i]}:\", size_of_mask)\n",
        "    # cv2.imwrite(f'mask-{i}.png', mask_rgba)\n",
        "    ax.imshow(mask_rgba, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319d675a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "\n",
        "from train import get_model_instance_segmentation, get_transform\n",
        "\n",
        "image = read_image(\"../data/frames/madden-001-mask.png\")\n",
        "\n",
        "model = get_model_instance_segmentation(3)\n",
        "model.load_state_dict(torch.load('../model.pth'))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "eval_transform = get_transform(train=False)\n",
        "device = torch.device(\n",
        "        'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    x = eval_transform(image)\n",
        "    # convert RGBA -> RGB and move to device\n",
        "    x = x[:3, ...].to(device)\n",
        "    predictions = model([x, ])\n",
        "    pred = predictions[0]\n",
        "\n",
        "\n",
        "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
        "image = image[:3, ...]\n",
        "pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
        "pred_boxes = pred[\"boxes\"].long()\n",
        "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
        "\n",
        "masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
        "output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(output_image.permute(1, 2, 0))\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
