{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa10fcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df82d10d-56a5-456c-bb98-c9a8b572e0a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "image = read_image(\"../data/frames/pressbox-001.jpg\")\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df95281",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def quantize_image(image, n_colors):\n",
        "    channels, height, width,  = image.shape\n",
        "\n",
        "    # Reshape image to be a list of pixels\n",
        "    pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "    print(\"Pixels shape:\", pixels.shape) # [16384, 3]\n",
        "\n",
        "    unique_colors = torch.unique(pixels, dim=0) # Include all the unique colors\n",
        "    sampled_colors = shuffle(pixels, random_state=0, n_samples=100) # Sample 100 colors for frequency.\n",
        "    combined_colors = torch.vstack([unique_colors, sampled_colors]) # Combine the two tensors.\n",
        "\n",
        "    # Apply KMeans\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(combined_colors)\n",
        "    labels = kmeans.predict(pixels)\n",
        "    quantized = torch.from_numpy(kmeans.cluster_centers_[labels])\n",
        "    print(\"Quantized Shape:\", quantized.shape)\n",
        "    quantized = quantized.permute(1, 0).reshape(channels, height, width)\n",
        "    return quantized\n",
        "\n",
        "\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "mask = mask / 255.0\n",
        "print(\"Mask shape:\", mask.shape)\n",
        "quantized_mask = quantize_image(mask, 24)\n",
        "print(\"Quantized mask shape:\", quantized_mask.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Quantized\")\n",
        "plt.imshow(quantized_mask.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e26028",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Define the size of the image\n",
        "height = 64\n",
        "width = 128\n",
        "\n",
        "# Create a tensor with values increasing linearly between 0 and 1\n",
        "image_hsv = torch.zeros((3, height, width))\n",
        "image_hsv[0] = torch.linspace(0, 1, height * width).view(height, width)\n",
        "\n",
        "# Set saturation and value to 1\n",
        "image_hsv[1] = 1\n",
        "image_hsv[2] = 1\n",
        "\n",
        "# Convert the image to RGB\n",
        "image_rgb = mcolors.hsv_to_rgb(image_hsv.permute(1, 2, 0).numpy())\n",
        "\n",
        "# Convert back to tensor\n",
        "image_rgb_tensor = torch.from_numpy(image_rgb)\n",
        "print(\"RGB Image Shape:\", image_rgb_tensor.shape)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image_rgb_tensor)\n",
        "plt.subplot(122)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(quantize_image(image_rgb_tensor.permute(2, 0, 1), 8).permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f308c37d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Shape:\", image_rgb_tensor.shape)\n",
        "pixels = image_rgb_tensor.reshape(-1, 3)\n",
        "print(\"Shape:\", pixels.shape)\n",
        "print(\"First 10 pixels:\\n\", pixels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc41ba75",
      "metadata": {},
      "outputs": [],
      "source": [
        "target_dir = \"../data/frames/\"\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "for filename in os.listdir(target_dir):\n",
        "    if filename.endswith('-mask.jpg'):\n",
        "        # Open image and quantize it\n",
        "        image = Image.open(os.path.join(target_dir, filename))\n",
        "        image = quantize_image(image, 24)\n",
        "\n",
        "        # Save quantized image as PNG\n",
        "        new_filename = os.path.splitext(filename)[0] + '.png'\n",
        "        image.save(os.path.join(target_dir, new_filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9bd545f-9264-4f98-bdca-36889145685e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "\n",
        "class FootballDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, frame_directory, transforms):\n",
        "        self.frame_directory = frame_directory\n",
        "        self.transforms = transforms\n",
        "        images = list(sorted(os.listdir(frame_directory)))\n",
        "        self.imgs = [img for img in images if not img.endswith(\"-mask.jpg\")]\n",
        "        self.masks = [img for img in images if img.endswith(\"-mask.jpg\")]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.frame_directory, self.imgs[idx])\n",
        "        mask_path = os.path.join(self.frame_directory, self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
