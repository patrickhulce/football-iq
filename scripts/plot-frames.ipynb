{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa10fcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df82d10d-56a5-456c-bb98-c9a8b572e0a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "image = read_image(\"../data/frames/pressbox-001.jpg\")\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.subplot(122)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(mask.permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df95281",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def quantize_image(image, n_colors):\n",
        "    channels, height, width,  = image.shape\n",
        "\n",
        "    # Reshape image to be a list of pixels\n",
        "    pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "    print(\"Pixels shape:\", pixels.shape) # [16384, 3]\n",
        "\n",
        "    unique_colors = torch.unique(pixels, dim=0) # Include all the unique colors\n",
        "    sampled_colors = shuffle(pixels, random_state=0, n_samples=100) # Sample 100 colors for frequency.\n",
        "    combined_colors = torch.vstack([unique_colors, sampled_colors]) # Combine the two tensors.\n",
        "\n",
        "    # Apply KMeans\n",
        "    kmeans = KMeans(n_clusters=n_colors).fit(combined_colors)\n",
        "    labels = kmeans.predict(pixels)\n",
        "    quantized = torch.from_numpy(kmeans.cluster_centers_[labels])\n",
        "    print(\"Quantized Shape:\", quantized.shape)\n",
        "    quantized = quantized.permute(1, 0).reshape(channels, height, width)\n",
        "    return quantized\n",
        "\n",
        "def blur_image_manual(image: torch.Tensor, kernel_size=3):\n",
        "    # Define the blur kernel\n",
        "    blur_kernel = torch.ones(1, 1, kernel_size, kernel_size) / (kernel_size * kernel_size)\n",
        "    blur_kernel = blur_kernel.repeat(image.shape[0], 1, 1, 1)  # Repeat for each input channel\n",
        "\n",
        "    # Add an extra dimension to the image tensor and apply blur\n",
        "    image = image.unsqueeze(0)  # Add extra dimension for batch size\n",
        "    blurred_image = F.conv2d(image, blur_kernel, padding=1, groups=3)\n",
        "\n",
        "    # Remove the extra dimension\n",
        "    return blurred_image.squeeze(0)\n",
        "\n",
        "def blur_image(image: torch.Tensor, kernel_size=3):\n",
        "    return transforms.GaussianBlur(kernel_size=kernel_size)(image)\n",
        "\n",
        "def morphological_closing(image: torch.Tensor, kernel_size=2):\n",
        "    # Convert the tensor to a NumPy array\n",
        "    image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
        "    img = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Define a kernel for the morphological operation\n",
        "    kernel = np.ones((kernel_size,kernel_size), np.uint8)  # you may need to adjust the size\n",
        "\n",
        "    # Get all unique colors in the image\n",
        "    unique_colors = np.unique(img.reshape(-1, img.shape[2]), axis=0)\n",
        "\n",
        "    # Perform morphological closing for each unique color\n",
        "    cleaned_img = np.zeros_like(img)\n",
        "    for color in unique_colors:\n",
        "        # Create a binary mask for the current color\n",
        "        print(\"Processing color:\", color)\n",
        "        mask = (img == color).all(axis=2).astype(np.uint8)\n",
        "\n",
        "        # Perform morphological closing on the mask\n",
        "        closed_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "        # closed_mask = cv2.morphologyEx(closed_mask, cv2.MORPH_OPEN, kernel)\n",
        "        # closed_mask = cv2.dilate(closed_mask, kernel, iterations=1)\n",
        "\n",
        "        # Only keep the largest connected component by pixel count.\n",
        "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(closed_mask, connectivity=8)\n",
        "        # Order the labels by number of active pixels.\n",
        "        sorted_labels = np.argsort(stats[:, cv2.CC_STAT_AREA])\n",
        "        max_label = sorted_labels[-2]\n",
        "\n",
        "        for label in range(1, num_labels):\n",
        "            label_area = stats[label, cv2.CC_STAT_AREA]\n",
        "            if label_area < 50:\n",
        "                continue\n",
        "            # If the component isn't large enough, remove it from the cleaned image\n",
        "            print(\"Component area of {} comapred to max area of {}\".format(stats[label, cv2.CC_STAT_AREA], stats[max_label, cv2.CC_STAT_AREA]))\n",
        "            if label_area < stats[max_label, cv2.CC_STAT_AREA]:\n",
        "                closed_mask[labels == label] = 0\n",
        "\n",
        "        # Add the closed mask to the cleaned image\n",
        "        cleaned_img[closed_mask == 1] = color\n",
        "\n",
        "    # Convert the cleaned NumPy array back to a tensor\n",
        "    cleaned_tensor = torch.from_numpy(cleaned_img).permute(2, 0, 1)\n",
        "\n",
        "    return cleaned_tensor / 255.0\n",
        "\n",
        "def quantize_image_by_popularity(image, min_popularity: int = 150, use_blur: bool = False, use_morphology: bool = True):\n",
        "    channels, height, width = image.shape\n",
        "\n",
        "    # Reshape image to be a list of pixels\n",
        "    # image = (image * 255).clamp(0, 255).to(torch.int32)\n",
        "    # image = blur_image(image, kernel_size=3) if use_blur else image\n",
        "    # pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "    # print(\"Pixels shape:\", pixels.shape) # [16384, 3]\n",
        "\n",
        "    # # Count the number of times each color appears in the image.\n",
        "    # pixel_ints = pixels\n",
        "\n",
        "    image = blur_image(image, kernel_size=3) if use_blur else image\n",
        "    pixels = image.reshape(channels, -1).permute(1, 0)\n",
        "\n",
        "    # Count the number of times each color appears in the image.\n",
        "    pixel_ints = (pixels * 255).to(torch.int32)\n",
        "    # Treat any pixel with R, G, and B combined <100 as black.\n",
        "    pixel_ints[pixel_ints.sum(dim=1) < 100] = 0\n",
        "    # Treat any pixel with R, G, and B combined >710 as white.\n",
        "    pixel_ints[pixel_ints.sum(dim=1) > 710] = 255\n",
        "\n",
        "    pixel_merged = pixel_ints[:, 0] * 256 * 256 + pixel_ints[:, 1] * 256 + pixel_ints[:, 2]\n",
        "\n",
        "\n",
        "    print(\"Pixels merged shape:\", pixel_merged.shape)\n",
        "    print(\"Pixels merged:\", pixel_merged[:10])\n",
        "    unique_color_merged_counts = torch.bincount(pixel_merged)\n",
        "    print(\"Unique color merged counts shape:\", unique_color_merged_counts.shape)\n",
        "    print(\"Unique color merged counts:\", unique_color_merged_counts[:10])\n",
        "    sorted_indices = unique_color_merged_counts.argsort(descending=True)\n",
        "    top_colors_merged = sorted_indices[:30]\n",
        "    # Restrict top colors to those that appear at least min_popularity times.\n",
        "    top_color_counts = unique_color_merged_counts[top_colors_merged]\n",
        "    top_colors_merged = top_colors_merged[top_color_counts >= min_popularity]\n",
        "    top_colors = [(color.item() // (256 * 256), (color.item() // 256) % 256, color.item() % 256) for color in top_colors_merged] + [(255, 255, 255)]\n",
        "    top_colors_tensor = torch.tensor(top_colors, dtype=torch.float32) / 255  # Convert to tensor and normalize to [0, 1]\n",
        "    print(\"Top colors:\", top_colors)\n",
        "    print(\"Top color counts:\", top_color_counts)\n",
        "\n",
        "    # Remap each pixel in the image to the closest of the top colors.\n",
        "    distances = torch.norm(pixels.unsqueeze(1) - top_colors_tensor, dim=2)  # Calculate distances to top colors\n",
        "    closest = distances.argmin(dim=1)  # Find the index of the smallest distance\n",
        "    remapped_pixels = top_colors_tensor[closest]  # Use this index to get the corresponding top color\n",
        "\n",
        "    # Reshape the remapped pixels to the original image shape\n",
        "    remapped_image = remapped_pixels.permute(1, 0).reshape(channels, height, width)\n",
        "    remapped_image = morphological_closing(remapped_image, kernel_size=5) if use_morphology else image\n",
        "    return remapped_image\n",
        "\n",
        "def quantized_by_edges(image):\n",
        "    # Convert the tensor to a NumPy array\n",
        "    image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
        "    img = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Split the image into R, G, B channels\n",
        "    r, g, b = cv2.split(img)\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Perform Canny edge detection on each channel\n",
        "    bottom_threshold = 50\n",
        "    top_threshold = 125\n",
        "    edges_k = cv2.Canny(img_gray, bottom_threshold, top_threshold)\n",
        "    edges_r = cv2.Canny(r, bottom_threshold, top_threshold)\n",
        "    edges_g = cv2.Canny(g, bottom_threshold, top_threshold)\n",
        "    edges_b = cv2.Canny(b, bottom_threshold, top_threshold)\n",
        "\n",
        "    # Combine the edges from all channels\n",
        "    _, edges = cv2.threshold(edges_k | edges_r | edges_g | edges_b, 50, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Calculate the intensity of the original image\n",
        "    intensities = np.mean(img, axis=2)\n",
        "    # Create a mask of dark colors in the original image\n",
        "    bg_mask = intensities < 30\n",
        "\n",
        "    # Perform flood fill from the top-left corner to make the background black\n",
        "    edges = cv2.bitwise_not(edges)\n",
        "    edges[bg_mask == 1] = 0\n",
        "\n",
        "    # Find connected components from edges\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(edges, connectivity=4)\n",
        "\n",
        "    # Create an empty image to store the result\n",
        "    result = np.zeros_like(img)\n",
        "\n",
        "    # Iterate over each label found in the image\n",
        "    for label in range(1, num_labels):\n",
        "        should_print = False\n",
        "        if stats[label, cv2.CC_STAT_LEFT] > 400 and stats[label, cv2.CC_STAT_LEFT] < 800 and stats[label, cv2.CC_STAT_TOP] > 400 and stats[label, cv2.CC_STAT_TOP] < 800:\n",
        "            should_print = True\n",
        "\n",
        "        log = lambda *args: print(*args) if should_print else None\n",
        "        if stats[label, cv2.CC_STAT_AREA] < 30:\n",
        "            continue\n",
        "        edge_mask = (labels == label).astype(np.uint8)\n",
        "\n",
        "        # Get the bounding box of the current component\n",
        "        x, y, w, h = stats[label, cv2.CC_STAT_LEFT], stats[label, cv2.CC_STAT_TOP], \\\n",
        "                     stats[label, cv2.CC_STAT_WIDTH], stats[label, cv2.CC_STAT_HEIGHT]\n",
        "        log(\"Processing label {} with bounding box ({}, {}, {}, {})\".format(label, x, y, w, h))\n",
        "        mask = np.zeros_like(edges, dtype=np.uint8)\n",
        "        mask[y-1:y+h+1, x-1:x+w+1] = 1\n",
        "        log(\"Image shape:\", img.shape)\n",
        "        log(\"Mask shape:\", mask.shape)\n",
        "\n",
        "        # Get the colors of the component\n",
        "        colors_r = img[:,:,0][mask == 1]\n",
        "        colors_g = img[:,:,1][mask == 1]\n",
        "        colors_b = img[:,:,2][mask == 1]\n",
        "\n",
        "        # Stack the colors together along the last axis\n",
        "        colors = img[edge_mask == 1]\n",
        "        log(\"Colors shape:\", colors.shape)\n",
        "\n",
        "        # Find unique colors and their counts\n",
        "        unique_colors, counts = np.unique(colors, return_counts=True, axis=0)\n",
        "        # Calculate the intensity of each unique color\n",
        "        intensities = np.mean(unique_colors, axis=1)\n",
        "        # Create a mask that includes only the colors with intensity above a certain threshold\n",
        "        colors_mask = intensities > 30\n",
        "        # Apply the mask to unique_colors and counts\n",
        "        unique_colors = unique_colors[colors_mask]\n",
        "        counts = counts[colors_mask]\n",
        "\n",
        "        # Get the color with the maximum count\n",
        "        most_common_color = unique_colors[np.argmax(counts)]\n",
        "\n",
        "        # Convert most_common_color to tuple for floodFill\n",
        "        color_tuple = (int(most_common_color[0]), int(most_common_color[1]), int(most_common_color[2]))\n",
        "\n",
        "        # Get the seed point\n",
        "        seed_point = (int(centroids[label][0]), int(centroids[label][1]))\n",
        "\n",
        "        # Create a mask for flood filling that is 2 pixels larger in each direction than mask.\n",
        "        flood_fill_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
        "        # now fill the same pixels as were in mask but shifted 1 pixel in each direction\n",
        "        flood_fill_mask[1:-1, 1:-1] = mask\n",
        "\n",
        "        log(f\"Seed point: {seed_point}, color: {color_tuple}\")\n",
        "        # Fill the component with the most common color in the result image\n",
        "        result[edge_mask == 1] = most_common_color\n",
        "        # Perform flood fill with the most common color\n",
        "        # cv2.floodFill(result, flood_fill_mask, seedPoint=seed_point, newVal=color_tuple)\n",
        "\n",
        "    # Convert the result NumPy array back to a tensor\n",
        "    result_tensor = torch.from_numpy(result).permute(2, 0, 1) / 255.0\n",
        "\n",
        "    result_tensor = morphological_closing(result_tensor, kernel_size=5)\n",
        "\n",
        "    # Convert edges to an RGB image\n",
        "    edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # Convert the RGB numpy array to a PyTorch tensor\n",
        "    edges_tensor = torch.from_numpy(edges_rgb).permute(2, 0, 1) / 255.0\n",
        "\n",
        "    # return edges_tensor\n",
        "    return result_tensor\n",
        "\n",
        "def image_for_display(image, crop=True):\n",
        "    # Define the crop coordinates\n",
        "    start_row = 500\n",
        "    start_col = 500\n",
        "    height = 200\n",
        "    width = 400\n",
        "\n",
        "    # Crop the image\n",
        "    cropped_img = image[:, start_row:start_row+height, start_col:start_col+width]\n",
        "\n",
        "    image = cropped_img if crop else image\n",
        "    return image.permute(1, 2, 0)\n",
        "\n",
        "mask = read_image(\"../data/frames/pressbox-001-mask.jpg\")\n",
        "mask = mask / 255.0\n",
        "print(\"Mask shape:\", mask.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.subplot(221)\n",
        "plt.title(\"Mask\")\n",
        "plt.imshow(image_for_display(mask))\n",
        "plt.subplot(222)\n",
        "plt.title(\"Blurred\")\n",
        "plt.imshow(image_for_display(blur_image(mask, kernel_size=3)))\n",
        "# plt.subplot(223)\n",
        "# plt.title(\"Quantized by popularity\")\n",
        "# plt.imshow(image_for_display(quantize_image_by_popularity(mask, min_popularity=300, use_morphology=True)))\n",
        "plt.subplot(224)\n",
        "plt.title(\"Quantized by edges\")\n",
        "plt.imshow(image_for_display(quantized_by_edges(mask)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e26028",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Define the size of the image\n",
        "height = 64\n",
        "width = 128\n",
        "\n",
        "# Create a tensor with values increasing linearly between 0 and 1\n",
        "image_hsv = torch.zeros((3, height, width))\n",
        "image_hsv[0] = torch.linspace(0, 1, height * width).view(height, width)\n",
        "\n",
        "# Set saturation and value to 1\n",
        "image_hsv[1] = 1\n",
        "image_hsv[2] = 1\n",
        "\n",
        "# Convert the image to RGB\n",
        "image_rgb = mcolors.hsv_to_rgb(image_hsv.permute(1, 2, 0).numpy())\n",
        "\n",
        "# Convert back to tensor\n",
        "image_rgb_tensor = torch.from_numpy(image_rgb)\n",
        "print(\"RGB Image Shape:\", image_rgb_tensor.shape)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(121)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(image_rgb_tensor)\n",
        "plt.subplot(122)\n",
        "plt.title(\"Image\")\n",
        "plt.imshow(quantize_image(image_rgb_tensor.permute(2, 0, 1), 8).permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f308c37d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Shape:\", image_rgb_tensor.shape)\n",
        "pixels = image_rgb_tensor.reshape(-1, 3)\n",
        "print(\"Shape:\", pixels.shape)\n",
        "print(\"First 10 pixels:\\n\", pixels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_dir = \"../data/frames/\"\n",
        "\n",
        "import os\n",
        "from torchvision.io import read_image, write_png\n",
        "from PIL import Image\n",
        "\n",
        "for filename in os.listdir(target_dir):\n",
        "    if filename.endswith('-mask.jpg'):\n",
        "        print(\"Processing\", filename, \"...\")\n",
        "        # Open image and quantize it\n",
        "        image = read_image(os.path.join(target_dir, filename))\n",
        "        image = quantized_by_edges(image / 255.0)\n",
        "\n",
        "        # Save quantized image as PNG\n",
        "        int_image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
        "\n",
        "        new_filename = os.path.splitext(filename)[0] + '.png'\n",
        "        write_png(int_image, os.path.join(target_dir, new_filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0242fb3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914afc80",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from torchvision.io import read_image\n",
        "import importlib\n",
        "import process_mask\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "importlib.reload(process_mask)\n",
        "\n",
        "mask = read_image(\"../data/frames/madden-001-mask.png\")\n",
        "target = process_mask.process_mask(mask, 1)\n",
        "\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(mask.permute(1, 2, 0))\n",
        "print(target['labels'])\n",
        "\n",
        "\n",
        "boxes = target[\"boxes\"].numpy()\n",
        "masks = target[\"masks\"].numpy()\n",
        "labels = target[\"labels\"].numpy()\n",
        "\n",
        "# Iterate over each object\n",
        "for i in range(len(labels)):\n",
        "    # Draw the bounding box\n",
        "    box = boxes[i]\n",
        "    rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    # Draw the mask\n",
        "    mask = masks[i]\n",
        "    mask_rgba = np.zeros((mask.shape[0], mask.shape[1], 4))  # Initialize with zeros\n",
        "    mask_rgba[masks[i] > 0, :3] = 1  # Set RGB channels to 1 where mask is not zero\n",
        "    mask_rgba[masks[i] > 0, 3] = 1  # Set Alpha channel to 1 where mask is not zero\n",
        "\n",
        "    size_of_mask = np.where(mask == True)[0].sum()\n",
        "\n",
        "    print(f\"Size of mask for label {labels[i]}:\", size_of_mask)\n",
        "    # cv2.imwrite(f'mask-{i}.png', mask_rgba)\n",
        "    ax.imshow(mask_rgba, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319d675a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "\n",
        "class FootballDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, frame_directory, transforms):\n",
        "        self.frame_directory = frame_directory\n",
        "        self.transforms = transforms\n",
        "        images = list(sorted(os.listdir(frame_directory)))\n",
        "        self.imgs = [img for img in images if not img.endswith(\"-mask.jpg\")]\n",
        "        self.masks = [img for img in images if img.endswith(\"-mask.jpg\")]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.frame_directory, self.imgs[idx])\n",
        "        mask_path = os.path.join(self.frame_directory, self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
